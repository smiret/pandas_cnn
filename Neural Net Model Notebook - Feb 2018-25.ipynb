{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network For Composite Materials Design - FEM Solver Trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for the effective property solutions from the FEM solver are written out to txt files. The inputs from the particles radii inform the outputs of the effective properties. This data can then be used to train a neural network as a proxy to solve the system of PDEs inside the Genetic Algorithm and thereby speed up the computation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to pre-process the data to yield the required layer of inputs and outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by finding the relevant markers and their positions by appending them into relevant lists and then write the process into a function to be able to iterate over the required lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def data_extract_txt(table_flat_list, etensor_flat_list, keff_flat_list, file_iter):\n",
    "    #Open the data file\n",
    "    f_test = open('mesoscale_ga_data_feb_17_datatest_n10_iter_%i.txt' % file_iter)\n",
    "\n",
    "    #append all lines to a list, index them and \n",
    "    #find the lines that contain the 'Next Table' for a particle table\n",
    "    # find the lines with 'Etensor_mat' and K_eff_mat to deliminate those matrices\n",
    "\n",
    "    #lists and counters\n",
    "    lines_list = []\n",
    "    counter_next_table = 0\n",
    "    counter_etensor = 0\n",
    "    counter_keff = 0\n",
    "    next_table_list = []\n",
    "    etensor_list = []\n",
    "    keff_list = []\n",
    "\n",
    "    #find the relevant line numbers\n",
    "    for line in f_test.readlines():\n",
    "        lines_list.append(line)\n",
    "        if 'Next Table' in line:\n",
    "            next_table_list.append(counter_next_table)\n",
    "        if 'Etensor_mat' in line:\n",
    "            etensor_list.append(counter_etensor)\n",
    "        if 'K_eff_mat' in line:\n",
    "            keff_list.append(counter_keff)\n",
    "\n",
    "        #increase the counters\n",
    "        counter_next_table += 1\n",
    "        counter_etensor += 1\n",
    "        counter_keff += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Extract the particle table - from the start of the the table list to the beginning of the Etensor list\n",
    "    num_structures = len(next_table_list)\n",
    "\n",
    "    fiber_table1 = np.zeros((150,4))\n",
    "    etensor1 = np.zeros((6,6))\n",
    "    keff1 = np.zeros((3,3))\n",
    "\n",
    "    for ii in range (0, num_structures):\n",
    "        #Determine the relevant ranges\n",
    "        #start and stops for the particle table\n",
    "        table_start = next_table_list[ii]\n",
    "        table_stop = etensor_list[ii]\n",
    "        #the etensor\n",
    "        etensor_start = etensor_list[ii] + 1\n",
    "        etensor_stop = keff_list[ii] - 1\n",
    "        #the k tensor\n",
    "        keff_start = keff_list[ii] + 1\n",
    "        if (ii == (num_structures -1)):\n",
    "            keff_stop = len(lines_list) - 2\n",
    "        else:\n",
    "            keff_stop = next_table_list[ii+1] - 1\n",
    "\n",
    "        #Contruction of the Particle Table\n",
    "\n",
    "        #first element in the table\n",
    "        table_1_split = lines_list[table_start].split()\n",
    "        #first element\n",
    "        parse1 = table_1_split[2].split(\"[\")\n",
    "        parse2 = parse1[1].split(\",\")\n",
    "        float1 = float(parse2[0])\n",
    "        float2 = float(table_1_split[3].split(\",\")[0])\n",
    "        float3 = float(table_1_split[4].split(\",\")[0])\n",
    "        float4 = float(table_1_split[5].split(\"]\")[0])\n",
    "\n",
    "        fiber_table1[0] = np.array([float1, float2, float3, float4])\n",
    "\n",
    "        #all other elements in the table\n",
    "        count_table = 1\n",
    "        for jj in range(table_start+1, table_stop):\n",
    "            table_2_split = lines_list[jj].split()\n",
    "            parse1 = table_2_split[0].split(\"[\")\n",
    "            parse2 = parse1[1].split(\",\")\n",
    "            float1 = float(parse2[0])\n",
    "            float2 = float(table_2_split[1].split(\",\")[0])\n",
    "            float3 = float(table_2_split[2].split(\",\")[0])\n",
    "            float4 = float(table_2_split[3].split(\"]\")[0])\n",
    "\n",
    "            fiber_table1[count_table] = np.array([float1, float2, float3, float4])\n",
    "\n",
    "            count_table += 1\n",
    "\n",
    "        #Construction of the Etensor\n",
    "        count_tensor = 0\n",
    "        for jj in range(etensor_start, etensor_stop):\n",
    "            tensor_1_split = lines_list[jj].split()\n",
    "            parse1 = tensor_1_split[0].split(\"[\")\n",
    "            parse2 = parse1[1].split(\",\")\n",
    "            float1 = float(parse2[0])\n",
    "            float2 = float(tensor_1_split[1].split(\",\")[0])\n",
    "            float3 = float(tensor_1_split[2].split(\",\")[0])\n",
    "            float4 = float(tensor_1_split[3].split(\",\")[0])\n",
    "            float5 = float(tensor_1_split[4].split(\",\")[0])\n",
    "            float6 = float(tensor_1_split[5].split(\"]\")[0])\n",
    "\n",
    "            etensor1[count_tensor] = np.array([float1, float2, float3, float4, float5, float6])\n",
    "\n",
    "            count_tensor += 1\n",
    "\n",
    "        #Construction of the Keff\n",
    "        count_keff = 0\n",
    "        for jj in range(keff_start, keff_stop):\n",
    "            keff_1_split = lines_list[jj].split()\n",
    "            parse1 = keff_1_split[0].split(\"[\")\n",
    "            parse2 = parse1[1].split(\",\")\n",
    "            float1 = float(parse2[0])\n",
    "            float2 = float(keff_1_split[1].split(\",\")[0])\n",
    "            float3 = float(keff_1_split[2].split(\"]\")[0])\n",
    "\n",
    "\n",
    "            keff1[count_keff] = np.array([float1, float2, float3])\n",
    "\n",
    "            count_keff += 1\n",
    "\n",
    "\n",
    "        #Flatten the table, etensor, and keff to prepare for input into the data frame\n",
    "        table_flat_list.append(fiber_table1.flatten())\n",
    "        etensor_flat_list.append(etensor1.flatten())\n",
    "        keff_flat_list.append(keff1.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#iterator for the txt file\n",
    "table_flat_list = []\n",
    "etensor_flat_list = []\n",
    "keff_flat_list = []\n",
    "\n",
    "for file_iter in range(1,5):\n",
    "\n",
    "    data_extract_txt(table_flat_list, etensor_flat_list, keff_flat_list, file_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table_flat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required input for the neural net on scikit is in a the form of a table where each row represents a set of input and output values; therefore there are 600 columns for the inputs and 45 columns for the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#flatten inputs and outputs into one big data list\n",
    "pd_data_list = []\n",
    "\n",
    "num_structures = len(table_flat_list)\n",
    "\n",
    "for ii in range(0, num_structures):\n",
    "    list_line = np.array([np.array(table_flat_list[ii]), np.array(etensor_flat_list[ii]), np.array(keff_flat_list[ii])])\n",
    "    \n",
    "    #append to list\n",
    "    pd_data_list.append(list_line)\n",
    "\n",
    "#create pandas data frame\n",
    "data_frame_list = pd.DataFrame(pd_data_list, columns = ['Particle Table', 'Etensor', 'Keff'], dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Particle Table</th>\n",
       "      <th>Etensor</th>\n",
       "      <th>Keff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.529380427109, 0.0180876623966, 0.9397186344...</td>\n",
       "      <td>[515323206234.0, 142410309173.0, 142295255807....</td>\n",
       "      <td>[94.6492968247, 0.0, 0.0, 0.0, 94.649561605, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.58514756792, 0.330246801131, 0.230286513296...</td>\n",
       "      <td>[515906164747.0, 142625196470.0, 142221215287....</td>\n",
       "      <td>[94.6493096109, 0.0, 0.0, 0.0, 94.6495583475, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.449807278627, 0.554712899686, 0.61702651979...</td>\n",
       "      <td>[520355550893.0, 143678521742.0, 142997571282....</td>\n",
       "      <td>[94.6605856501, 0.0, 0.0, 0.0, 94.6607124519, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.489593852868, 0.286400281041, 0.77837257713...</td>\n",
       "      <td>[529690596238.0, 146720247572.0, 146827158820....</td>\n",
       "      <td>[94.7272209259, 0.0, 0.0, 0.0, 94.7275333902, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.557263997515, 0.174167231764, 0.58500257389...</td>\n",
       "      <td>[539427682806.0, 148189965084.0, 148247309752....</td>\n",
       "      <td>[94.7499684317, 0.0, 0.0, 0.0, 94.7495322182, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Particle Table  \\\n",
       "0  [0.529380427109, 0.0180876623966, 0.9397186344...   \n",
       "1  [0.58514756792, 0.330246801131, 0.230286513296...   \n",
       "2  [0.449807278627, 0.554712899686, 0.61702651979...   \n",
       "3  [0.489593852868, 0.286400281041, 0.77837257713...   \n",
       "4  [0.557263997515, 0.174167231764, 0.58500257389...   \n",
       "\n",
       "                                             Etensor  \\\n",
       "0  [515323206234.0, 142410309173.0, 142295255807....   \n",
       "1  [515906164747.0, 142625196470.0, 142221215287....   \n",
       "2  [520355550893.0, 143678521742.0, 142997571282....   \n",
       "3  [529690596238.0, 146720247572.0, 146827158820....   \n",
       "4  [539427682806.0, 148189965084.0, 148247309752....   \n",
       "\n",
       "                                                Keff  \n",
       "0  [94.6492968247, 0.0, 0.0, 0.0, 94.649561605, -...  \n",
       "1  [94.6493096109, 0.0, 0.0, 0.0, 94.6495583475, ...  \n",
       "2  [94.6605856501, 0.0, 0.0, 0.0, 94.6607124519, ...  \n",
       "3  [94.7272209259, 0.0, 0.0, 0.0, 94.7275333902, ...  \n",
       "4  [94.7499684317, 0.0, 0.0, 0.0, 94.7495322182, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the data for the neural net, split up into training data, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_frame_list['Particle Table']\n",
    "Y_mech = data_frame_list['Etensor']\n",
    "Y_therm = data_frame_list['Keff']\n",
    "\n",
    "#Mechanical\n",
    "x_train_mech, x_test_mech, y_train_mech, y_test_mech = train_test_split(X, Y_mech)\n",
    "#Thermal\n",
    "x_train_therm, x_test_therm, y_train_therm, y_test_therm = train_test_split(X, Y_therm)\n",
    "\n",
    "\n",
    "\n",
    "#convert the list of arrays back to a numpy array!!! BITHESSSSS\n",
    "# np1 = np.vstack(x_train)\n",
    "\n",
    "# np1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the standard scalar to pre-process the data with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# #fit only to the training data\n",
    "scaler.fit(np.vstack(x_train_mech))\n",
    "scaler.fit(np.vstack(x_train_therm))\n",
    "\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "x_train_mech = scaler.transform(np.vstack(x_train_mech))\n",
    "x_test_mech = scaler.transform(np.vstack(x_test_mech))\n",
    "x_train_therm = scaler.transform(np.vstack(x_train_therm))\n",
    "x_test_therm = scaler.transform(np.vstack(x_test_therm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(500, 400, 300, 200, 100, 50),\n",
       "       learning_rate='constant', learning_rate_init=0.001,\n",
       "       max_iter=5000000, momentum=0.9, nesterovs_momentum=True,\n",
       "       power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "       tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_etensor = MLPRegressor(hidden_layer_sizes=(500,400,300,200,100), max_iter = 5000000)\n",
    "mlp_etensor.fit(np.vstack(x_train_mech),np.vstack(y_train_mech))\n",
    "\n",
    "mlp_keff = MLPRegressor(hidden_layer_sizes=(500,400,300,200,100,50), max_iter = 5000000)\n",
    "mlp_keff.fit(np.vstack(x_train_therm),np.vstack(y_train_therm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predictions and Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the predict feature in the scikit learn tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "predictions_mech = mlp_etensor.predict(np.vstack(x_test_mech))\n",
    "predictions_therm = mlp_keff.predict(np.vstack(x_test_therm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9899604471368846e+21"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(predictions_mech, np.vstack(y_test_mech))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.898500381947045"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(predictions_therm, np.vstack(y_test_therm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_func(predictions, test_data):\n",
    "    error = 0.0\n",
    "    for ii in range(0, len(test_data)):\n",
    "        error += np.linalg.norm((predictions[ii]-test_data[ii]))/np.linalg.norm(test_data[ii])\n",
    "        \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.18150505e+11,   1.43194368e+11,   1.43267778e+11,\n",
       "        -8.29882272e+07,  -1.82065991e+07,   1.83635666e+08,\n",
       "         1.43194351e+11,   5.20006377e+11,   1.43524168e+11,\n",
       "        -2.37004519e+08,   3.93470506e+08,   4.07874126e+07,\n",
       "         1.43267709e+11,   1.43524160e+11,   5.20547566e+11,\n",
       "        -5.57958085e+07,   2.58925372e+08,   8.36956068e+07,\n",
       "        -1.46050955e+08,  -2.33738872e+08,  -6.20594523e+07,\n",
       "         1.90152137e+11,   3.13759696e+07,   8.50625828e+07,\n",
       "        -3.44964006e+07,   3.81108790e+08,   2.60268144e+08,\n",
       "         3.93800079e+07,   1.90298264e+11,  -2.42511590e+08,\n",
       "         1.67730951e+08,   4.16834909e+07,   8.17005345e+07,\n",
       "         8.48078245e+07,  -1.39451328e+08,   1.90115453e+11])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_mech[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.44508910e+11,   1.50398777e+11,   1.51305792e+11,\n",
       "        -2.64901322e+08,   7.28642326e+07,   6.53537902e+08,\n",
       "         1.50398777e+11,   5.45882169e+11,   1.51493918e+11,\n",
       "        -4.74130250e+08,   1.22943502e+09,   3.87286176e+08,\n",
       "         1.51305792e+11,   1.51493918e+11,   5.52473252e+11,\n",
       "        -1.31106227e+08,   1.51237588e+09,   3.02839294e+08,\n",
       "        -2.64901322e+08,  -4.74130250e+08,  -1.31106227e+08,\n",
       "         2.01712653e+11,  -3.74983509e+07,   2.18167540e+08,\n",
       "         7.28642326e+07,   1.22943502e+09,   1.51237588e+09,\n",
       "        -3.74983509e+07,   2.03444619e+11,  -7.96441722e+07,\n",
       "         6.53537902e+08,   3.87286176e+08,   3.02839294e+08,\n",
       "         2.18167540e+08,  -7.96441722e+07,   2.02354566e+11])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack(y_test_mech)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5286513932726207"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_func(predictions_mech, np.vstack(y_test_mech))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Save the model using the pickel feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 80.11102966,   1.09318754,   0.19883517, -10.78158052,\n",
       "        90.38448289, -11.85766343,   7.29640899,  -2.01917847,  78.26026394])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_therm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.47497741e+01,   0.00000000e+00,   2.84217094e-14,\n",
       "         0.00000000e+00,   9.47493095e+01,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   9.47499347e+01])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack(y_test_therm)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3962803417568539"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_func(predictions_therm, np.vstack(y_test_therm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "small_cnn_mech = mlp_etensor\n",
    "model_name_mech = 'small_neural_net_test_etensor.sav'\n",
    "pickle.dump(small_cnn_mech, open(model_name_mech, 'wb'))\n",
    "\n",
    "small_cnn_therm = mlp_keff\n",
    "model_name_therm = 'small_neural_net_test_keff.sav'\n",
    "pickle.dump(small_cnn_therm, open(model_name_therm, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
