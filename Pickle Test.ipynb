{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the saved model using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Santiago/anaconda2/lib/python2.7/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MLPRegressor from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "load_cnn_mech = pickle.load(open('small_neural_net_test_etensor_feb25_2018_n80.sav'))\n",
    "load_cnn_keff = pickle.load(open('small_neural_net_test_keff_feb25_2018_n80.sav'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def data_extract_txt(table_flat_list, etensor_flat_list, keff_flat_list, file_iter):\n",
    "    #Open the data file\n",
    "    f_test = open('mesoscale_ga_data_feb_17_datatest_n10_iter_%i.txt' % file_iter)\n",
    "\n",
    "    #append all lines to a list, index them and \n",
    "    #find the lines that contain the 'Next Table' for a particle table\n",
    "    # find the lines with 'Etensor_mat' and K_eff_mat to deliminate those matrices\n",
    "\n",
    "    #lists and counters\n",
    "    lines_list = []\n",
    "    counter_next_table = 0\n",
    "    counter_etensor = 0\n",
    "    counter_keff = 0\n",
    "    next_table_list = []\n",
    "    etensor_list = []\n",
    "    keff_list = []\n",
    "\n",
    "    #find the relevant line numbers\n",
    "    for line in f_test.readlines():\n",
    "        lines_list.append(line)\n",
    "        if 'Next Table' in line:\n",
    "            next_table_list.append(counter_next_table)\n",
    "        if 'Etensor_mat' in line:\n",
    "            etensor_list.append(counter_etensor)\n",
    "        if 'K_eff_mat' in line:\n",
    "            keff_list.append(counter_keff)\n",
    "\n",
    "        #increase the counters\n",
    "        counter_next_table += 1\n",
    "        counter_etensor += 1\n",
    "        counter_keff += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Extract the particle table - from the start of the the table list to the beginning of the Etensor list\n",
    "    num_structures = len(next_table_list)\n",
    "\n",
    "    fiber_table1 = np.zeros((150,4))\n",
    "    etensor1 = np.zeros((6,6))\n",
    "    keff1 = np.zeros((3,3))\n",
    "\n",
    "    for ii in range (0, num_structures):\n",
    "        #Determine the relevant ranges\n",
    "        #start and stops for the particle table\n",
    "        table_start = next_table_list[ii]\n",
    "        table_stop = etensor_list[ii]\n",
    "        #the etensor\n",
    "        etensor_start = etensor_list[ii] + 1\n",
    "        etensor_stop = keff_list[ii] - 1\n",
    "        #the k tensor\n",
    "        keff_start = keff_list[ii] + 1\n",
    "        if (ii == (num_structures -1)):\n",
    "            keff_stop = len(lines_list) - 2\n",
    "        else:\n",
    "            keff_stop = next_table_list[ii+1] - 1\n",
    "\n",
    "        #Contruction of the Particle Table\n",
    "\n",
    "        #first element in the table\n",
    "        table_1_split = lines_list[table_start].split()\n",
    "        #first element\n",
    "        parse1 = table_1_split[2].split(\"[\")\n",
    "        parse2 = parse1[1].split(\",\")\n",
    "        float1 = float(parse2[0])\n",
    "        float2 = float(table_1_split[3].split(\",\")[0])\n",
    "        float3 = float(table_1_split[4].split(\",\")[0])\n",
    "        float4 = float(table_1_split[5].split(\"]\")[0])\n",
    "\n",
    "        fiber_table1[0] = np.array([float1, float2, float3, float4])\n",
    "\n",
    "        #all other elements in the table\n",
    "        count_table = 1\n",
    "        for jj in range(table_start+1, table_stop):\n",
    "            table_2_split = lines_list[jj].split()\n",
    "            parse1 = table_2_split[0].split(\"[\")\n",
    "            parse2 = parse1[1].split(\",\")\n",
    "            float1 = float(parse2[0])\n",
    "            float2 = float(table_2_split[1].split(\",\")[0])\n",
    "            float3 = float(table_2_split[2].split(\",\")[0])\n",
    "            float4 = float(table_2_split[3].split(\"]\")[0])\n",
    "\n",
    "            fiber_table1[count_table] = np.array([float1, float2, float3, float4])\n",
    "\n",
    "            count_table += 1\n",
    "\n",
    "        #Construction of the Etensor\n",
    "        count_tensor = 0\n",
    "        for jj in range(etensor_start, etensor_stop):\n",
    "            tensor_1_split = lines_list[jj].split()\n",
    "            parse1 = tensor_1_split[0].split(\"[\")\n",
    "            parse2 = parse1[1].split(\",\")\n",
    "            float1 = float(parse2[0])\n",
    "            float2 = float(tensor_1_split[1].split(\",\")[0])\n",
    "            float3 = float(tensor_1_split[2].split(\",\")[0])\n",
    "            float4 = float(tensor_1_split[3].split(\",\")[0])\n",
    "            float5 = float(tensor_1_split[4].split(\",\")[0])\n",
    "            float6 = float(tensor_1_split[5].split(\"]\")[0])\n",
    "\n",
    "            etensor1[count_tensor] = np.array([float1, float2, float3, float4, float5, float6])\n",
    "\n",
    "            count_tensor += 1\n",
    "\n",
    "        #Construction of the Keff\n",
    "        count_keff = 0\n",
    "        for jj in range(keff_start, keff_stop):\n",
    "            keff_1_split = lines_list[jj].split()\n",
    "            parse1 = keff_1_split[0].split(\"[\")\n",
    "            parse2 = parse1[1].split(\",\")\n",
    "            float1 = float(parse2[0])\n",
    "            float2 = float(keff_1_split[1].split(\",\")[0])\n",
    "            float3 = float(keff_1_split[2].split(\"]\")[0])\n",
    "\n",
    "\n",
    "            keff1[count_keff] = np.array([float1, float2, float3])\n",
    "\n",
    "            count_keff += 1\n",
    "\n",
    "\n",
    "        #Flatten the table, etensor, and keff to prepare for input into the data frame\n",
    "        table_flat_list.append(fiber_table1.flatten())\n",
    "        etensor_flat_list.append(etensor1.flatten())\n",
    "        keff_flat_list.append(keff1.flatten())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#iterator for the txt file\n",
    "table_flat_list = []\n",
    "etensor_flat_list = []\n",
    "keff_flat_list = []\n",
    "\n",
    "for file_iter in range(1,5):\n",
    "\n",
    "    data_extract_txt(table_flat_list, etensor_flat_list, keff_flat_list, file_iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#flatten inputs and outputs into one big data list\n",
    "pd_data_list = []\n",
    "\n",
    "num_structures = len(table_flat_list)\n",
    "\n",
    "for ii in range(0, num_structures):\n",
    "    list_line = np.array([np.array(table_flat_list[ii]), np.array(etensor_flat_list[ii]), np.array(keff_flat_list[ii])])\n",
    "    \n",
    "    #append to list\n",
    "    pd_data_list.append(list_line)\n",
    "\n",
    "#create pandas data frame\n",
    "data_frame_list = pd.DataFrame(pd_data_list, columns = ['Particle Table', 'Etensor', 'Keff'], dtype='float')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_frame_list['Particle Table']\n",
    "Y_mech = data_frame_list['Etensor']\n",
    "Y_therm = data_frame_list['Keff']\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y)\n",
    "\n",
    "#convert the list of arrays back to a numpy array!!! BITHESSSSS\n",
    "# np1 = np.vstack(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_mech = load_cnn_mech.predict(np.vstack(X))\n",
    "predictions_therm = load_cnn_keff.predict(np.vstack(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_func(predictions, test_data):\n",
    "    error = 0.0\n",
    "    for ii in range(0, len(test_data)):\n",
    "        error += np.linalg.norm((predictions[ii]-test_data[ii]))/np.linalg.norm(test_data[ii])\n",
    "        \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.999015116695631"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_func(predictions_mech, np.vstack(Y_mech))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.741590453438565"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_func(predictions_therm, np.vstack(Y_therm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.43263515e+07,   3.95238958e+06,   3.95239631e+06, ...,\n",
       "          3.47478828e+03,  -1.41755283e+03,   5.23969616e+06],\n",
       "       [  1.43263515e+07,   3.95238958e+06,   3.95239631e+06, ...,\n",
       "          3.47478828e+03,  -1.41755283e+03,   5.23969616e+06],\n",
       "       [  1.43263515e+07,   3.95238958e+06,   3.95239631e+06, ...,\n",
       "          3.47478828e+03,  -1.41755283e+03,   5.23969616e+06],\n",
       "       ..., \n",
       "       [  1.43263515e+07,   3.95238958e+06,   3.95239631e+06, ...,\n",
       "          3.47478828e+03,  -1.41755283e+03,   5.23969616e+06],\n",
       "       [  1.43263515e+07,   3.95238958e+06,   3.95239631e+06, ...,\n",
       "          3.47478828e+03,  -1.41755283e+03,   5.23969616e+06],\n",
       "       [  1.43263515e+07,   3.95238958e+06,   3.95239631e+06, ...,\n",
       "          3.47478828e+03,  -1.41755283e+03,   5.23969616e+06]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_mech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [515323206234.0, 142410309173.0, 142295255807....\n",
       "1     [515906164747.0, 142625196470.0, 142221215287....\n",
       "2     [520355550893.0, 143678521742.0, 142997571282....\n",
       "3     [529690596238.0, 146720247572.0, 146827158820....\n",
       "4     [539427682806.0, 148189965084.0, 148247309752....\n",
       "5     [536371885475.0, 147493749061.0, 148463254256....\n",
       "6     [510362059412.0, 140897370963.0, 140938666443....\n",
       "7     [519970647542.0, 143634118520.0, 144187952850....\n",
       "8     [525418591278.0, 145544027888.0, 146277202998....\n",
       "9     [510362059412.0, 140897370963.0, 140938666443....\n",
       "10    [515323206234.0, 142410309173.0, 142295255807....\n",
       "11    [515906164747.0, 142625196470.0, 142221215287....\n",
       "12    [530077419177.0, 146352385512.0, 146728886298....\n",
       "13    [546206243372.0, 150572952686.0, 151091436775....\n",
       "14    [539427682806.0, 148189965084.0, 148247309752....\n",
       "15    [508609207344.0, 140675865290.0, 140479925744....\n",
       "16    [538112886801.0, 147912185650.0, 148395906640....\n",
       "17    [505649775104.0, 140518112258.0, 140120300332....\n",
       "18    [505649775104.0, 140518112258.0, 140120300332....\n",
       "19    [508609207344.0, 140675865290.0, 140479925744....\n",
       "20    [510362059412.0, 140897370963.0, 140938666443....\n",
       "21    [544508909654.0, 150398777199.0, 151305791790....\n",
       "22    [543936535924.0, 150100157263.0, 150523663013....\n",
       "23    [518603048513.0, 144352878707.0, 144360120336....\n",
       "24    [516442665331.0, 143131089423.0, 142098585659....\n",
       "25    [516376770769.0, 142461012663.0, 143215625556....\n",
       "26    [540875483958.0, 148311408803.0, 148998643707....\n",
       "27    [505649775104.0, 140518112258.0, 140120300332....\n",
       "28    [508609207344.0, 140675865290.0, 140479925744....\n",
       "29    [510362059412.0, 140897370963.0, 140938666443....\n",
       "30    [544508909654.0, 150398777199.0, 151305791790....\n",
       "31    [543936535924.0, 150100157263.0, 150523663013....\n",
       "32    [518603048513.0, 144352878707.0, 144360120336....\n",
       "33    [536636391426.0, 148668050080.0, 148149121701....\n",
       "34    [514362970167.0, 142364168168.0, 141518607542....\n",
       "35    [499034569219.0, 138291614506.0, 138640368962....\n",
       "Name: Etensor, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_mech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
